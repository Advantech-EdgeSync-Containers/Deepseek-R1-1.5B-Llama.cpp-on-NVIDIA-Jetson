# --- Model Settings ---

# Provide name for the GGUF model to be used
# Same name will be used for naming the model downloaded from hugging face
MODEL_NAME=DeepSeek-R1-1.5B.gguf

# --- Docker Compose File Settings---
COMPOSE_FILE_PATH=./docker-compose.yml

# --- Open Web UI Settings ---
OPENWEBUI_PORT=3000
OPENAI_API_LLAMA_CPP_BASE=http://localhost:11434/v1

# --- LLAMA CPP Settings ---
GPU_LAYERS=30
N_CTX=8192
N_BATCH=512

# Enables or disables chat title generation - default False
ENABLE_TITLE_GENERATION=False

# Enables or disables chat tag generation - default False
ENABLE_TAGS_GENERATION=False

# Hugging Face Model Downloader Configuration
# Update HF_REPO & HF_MODEL_FILE if user wants to try out some other HF gguf models
HF_TOKEN=<your-token>
HF_REPO="bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF"
HF_MODEL_FILE="DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf"

# Model Save Path
DEST_DIR="llama-cpp-service/models"
